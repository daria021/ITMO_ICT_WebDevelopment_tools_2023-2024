# Лабораторная работа: Потоки, процессы и асинхронность в Python

## Цель работы

Понять отличия между потоками и процессами и понять, что такое ассинхронность в Python.

Работа о потоках, процессах и асинхронности поможет студентам развить навыки создания эффективных и быстродействующих программ, что важно для работы с большими объемами данных и выполнения вычислений. Этот опыт также подготавливает студентов к реальным проектам, где требуется использование многопоточности и асинхронности для эффективной обработки данных или взаимодействия с внешними сервисами. Вопросы про потоки, процессы и ассинхронность встречаются, как минимум, на половине собеседований на python-разработчика уровня middle и Выше.

## Задание 1: Сравнение threading, multiprocessing и asyncio при вычислении суммы чисел

Необходимо вычислить сумму чисел от 1 до 10 000 000 . Реализовать три скрипта, каждый из которых выполняет эту задачу параллельно, используя:
1. Многопоточность (модуль `threading`)
2. Многопроцессность (модуль `multiprocessing`)
3. Асинхронность (модуль `asyncio`)


## Задача 1. Суммирование чисел от 1 до 1 000 000 000

В этой задаче мы разбили диапазон \[1…N\] на 4 равные части и посчитали сумму каждой части параллельно разными способами. После этого сложили частичные результаты и измерили общее время работы.


### 1. Threading (многопоточность)

**Код:** использует модуль `threading`.  
1. Делим диапазон на 4 отрезка.  
2. Для каждого отрезка создаём и запускаем `Thread(target=partial_sum, args=(start, end, idx))`.  
3. Каждый поток в своём цикле считает сумму чисел и кладёт результат в общий список `partial_sums[idx]`.  
4. В главном потоке ждём `join()` всех потоков, суммируем `partial_sums` и выводим время.

**Особенности подхода:**
- Потоки разделяют память процесса — результаты складываем в общий список без сериализации.
- GIL не даёт потокам реально исполняться на разных ядрах для CPU-bound задач, поэтому выигрыша по скорости нет.
- Подходит для I/O-bound задач, но не для тяжёлых вычислений.

---

### 2. Multiprocessing (многопроцессность)

**Код:** использует модуль `multiprocessing.Pool`.  
1. Делим диапазон на 4 кортежа `(start, end)`.  
2. Создаём `Pool(processes=4)` и вызываем `pool.starmap(partial_sum_range, ranges)`.  
3. Каждый процесс параллельно считает свою часть и возвращает частичную сумму.  
4. В главном процессе складываем все результаты и измеряем время.

**Особенности подхода:**
- Каждый процесс — независимый интерпретатор Python с собственным GIL, что позволяет настоящему параллелизму на многозадачных CPU.
- Обмен данных через возврат значений из `starmap`, не через общую память.
- Лучше всего подходит для CPU-bound задач.

---

### 3. AsyncIO (асинхронность)

**Код:** использует `asyncio` и корутины.  
1. Делим диапазон на 4 части.  
2. Для каждой части создаём корутину `partial_sum_async(start, end)`, внутри которой каждые 1 000 000 итераций вызывается `await asyncio.sleep(0)` чтобы уступить управление.  
3. Запускаем все корутины через `asyncio.create_task(...)` и ждём их вместе в `await asyncio.gather(...)`.  
4. Суммируем результаты и замеряем время.

**Особенности подхода:**
- Все корутины работают в одном потоке, и переключение между ними кооперативное.
- Для CPU-bound задач асинхронность не даёт ускорения: цикл событий блокируется тяжёлыми вычислениями.
- Эффективен толко в сочетании с реальными I/O-операциями.



### Результаты и сравнение (Задание 1)

Для оценки производительности запустим каждый из скриптов 
на диапазоне 1_000_000_000, чтобы уложиться в разумное время выполнения. 

| Подход            | Время выполнения          |
|-------------------|--------------------------------------|
| **Многопоточно** (`threading`, 4 потока) | 19.58 seconds (не ускоряется)      |
| **Многопроцессно** (`multiprocessing`, 4 процесса) | 6.51 seconds (примерно в 4 раза быстрее) |
| **AsyncIO** (4 задачи)   | 20.12 seconds (аналогично однопоточно)   |

### Комментарии к результатам

1. **Multiprocessing** показал **наибольшее ускорение**, так как каждый процесс работал на своём ядре без GIL-ограничений.  
2. **Threading** и **AsyncIO** дали близкие по скорости результаты: GIL и однопоточный event loop не позволяют распараллелить чисто вычислительный код.  
3. Для **CPU-bound** задач выбор очевиден: **multiprocessing**. Потоки и asyncio здесь избыточны и даже чуть медленнее (из-за накладных расходов).  


**Вывод по задаче 1:** Для вычислительно затратной задачи (суммирование большого количества чисел) модель многопроцессности выигрывает, так как задействует несколько ядер и обходит ограничение GIL. Многопоточное и асинхронное решения в CPython не могут ускорить такие расчёты и показали практически ту же скорость, что и последовательный код. Их использование для CPU-bound задачи неоправданно – сложность выше, а производительность не лучше. 

## Задание 2: Параллельный парсинг веб-страниц с сохранением в SQLite

 Реализовать три варианта скрипта, параллельно загружающего веб-страницы и извлекающего с них заголовки HTML (`<title>`), с последующим сохранением результатов в базу SQLite. Необходимо выполнить эти действия с использованием:
1. `threading` (потоки)
2. `multiprocessing` (процессы)
3. `asyncio` (асинхронные запросы, библиотека aiohttp)

В этом задании мы сделали парсинг одинакового списка из 15 URL с извлечением содержимого тега `<title>` и сохранили результаты в одну и ту же базу SQLite. Реализовали три подхода — `threading`, `multiprocessing` и `asyncio` — и сравнили время полного цикла «запрос → парсинг → запись».

### 1. Threading (многопоточность)

**Описание кода и подхода:**
- Для каждого URL создаётся поток `threading.Thread`, в котором выполняется синхронный HTTP-запрос `requests.get()`.
- Когда поток ждёт ответа от сервера, GIL автоматически освобождается, позволяя другим потокам работать.
- Заголовки страниц сохраняются в общий список `results`.
- После завершения всех потоков основной поток открывает соединение `sqlite3` и вставляет все записи одной операцией `executemany`.

**Особенности:**
- Потоки легко создавать и использовать: код почти не отличается от обычного синхронного.
- Подходит для задач с вводом-выводом (I/O-bound): несколько запросов выполняются одновременно.

---

### 2. Multiprocessing (многопроцессность)

**Описание кода и подхода:**
- Используется `multiprocessing.Pool` с числом процессов, равным числу URL (15).
- Каждый процесс запускает функцию `fetch_title(url)`, делает синхронный HTTP-запрос через `requests` и возвращает кортеж `(url, title)`.
- В родительском процессе после `pool.map` собираются все результаты и вставляются в SQLite.

**Особенности:**
- Каждый процесс — отдельный интерпретатор Python, свой GIL и свой системный контекст.
- Процессы работают параллельно на разных ядрах.
---

### 3. AsyncIO (асинхронность)

**Описание кода и подхода:**
- Используется `aiohttp.ClientSession` для асинхронных HTTP-запросов.
- Все 15 корутин `fetch_title(session, url)` запускаются через `asyncio.gather` в одном потоке.
- Благодаря `await` на сетевом вводе-выводе корутины переключаются одна на другую без создания потоков или процессов.
- Сохранение результатов выполняется асинхронно через `aiosqlite`, что не блокирует event loop.

**Особенности:**
- Минимальные накладные расходы на переключение между задачами.
- Отлично масштабируется на тысячи соединений, потребляет мало памяти.
- Эффективен исключительно для I/O-bound задач.

---

### Сравнение времени выполнения

| Подход            | Время выполнения, с |
| ----------------- | ------------------- |
| **Threading**     | 0.58                |
| **Multiprocessing** | 1.87               |
| **AsyncIO**       | 0.65                |

**Комментарий к результатам:**
- **Threading** показал наилучшее время (0.58 с): в случае I/O-bound задач синхронный клиент `requests` очень эффективен.
- **AsyncIO** отстал незначительно (0.65 с): асинхронный клиент `aiohttp` и использование `aiosqlite` дают небольшие накладные расходы на цикл событий, поэтому время чуть больше.
- **Multiprocessing** в данном сценарии самое медленное (1.87 с): создаётся 15 процессов (по одному на URL), что требует значительного времени на старт процессов и передачу их результатов в основной процесс.

**Вывод по задаче 2:** 
Параллельный парсинг страниц сильно быстрее последовательного. Из трёх подходов:

- **Threading** оказался самым быстрым при 15 ссылках — просто создаёшь потоки и сразу делаешь запросы.
- **AsyncIO** немного проиграл, но тоже быстро работает и легко масштабируется на большее число запросов.
- **Multiprocessing** в этой задаче медленнее из-за затрат на запуск процессов.

Итог: для сетевого парсинга лучше брать потоки или asyncio. Процессы здесь ни к чему.
## Общие выводы

- **Multiprocessing**: быстро обрабатывает тяжёлые вычисления на всех ядрах, но создаёт много процессов и ест больше памяти.  
- **Threading**: простой способ параллельно выполнять сетевые и файловые операции, но в тяжёлых вычислениях не ускоряет из-за GIL.  
- **AsyncIO**: отлично справляется с большим числом одновременных I/O-запросов без создания потоков, но не подходит для длительных вычислений.